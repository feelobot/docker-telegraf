# Telegraf configuration

# Telegraf is entirely plugin driven. All metrics are gathered from the
# declared plugins.

# Even if a plugin has no configuration, it must be declared in here
# to be active. Declaring a plugin means just specifying the name
# as a section with no variables. To deactivate a plugin, comment
# out the name and any variables.

# Use 'telegraf -config telegraf.toml -test' to see what metrics a config
# file would generate.

# One rule that plugins conform to is wherever a connection string
# can be passed, the values '' and 'localhost' are treated specially.
# They indicate to the plugin to use their own builtin configuration to
# connect to the local system.

# NOTE: The configuration has a few required parameters. They are marked
# with 'required'. Be sure to edit those to make this configuration work.

# Tags can also be specified via a normal map, but only one form at a time:
[tags]
  # dc = "us-east-1"

# Configuration for telegraf agent
[agent]
  # Default data collection interval for all plugins
  interval = "10s"
  # Rounds collection interval to 'interval'
  # ie, if interval="10s" then always collect on :00, :10, :20, etc.
  round_interval = true

  # Default data flushing interval for all outputs. You should not set this below
  # interval. Maximum flush_interval will be flush_interval + flush_jitter
  flush_interval = "10s"
  # Jitter the flush interval by a random amount. This is primarily to avoid
  # large write spikes for users running a large number of telegraf instances.
  # ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
  flush_jitter = "0s"

  # Run telegraf in debug mode
  debug = false
  # Override default hostname, if empty use os.Hostname()
  hostname = ""


###############################################################################
#                                  OUTPUTS                                    #
###############################################################################

[outputs]

# Configuration for Amon Server to send metrics to.
[[outputs.amon]]
  # Amon Server Key
  server_key = "my-server-key" # required.

  # Amon Instance URL
  amon_instance = "https://youramoninstance" # required

  # Connection timeout.
  # timeout = "5s"

# Configuration for the AMQP server to send metrics to
[[outputs.amqp]]
  # AMQP url
  url = "amqp://localhost:5672/influxdb"
  # AMQP exchange
  exchange = "telegraf"
  # Telegraf tag to use as a routing key
  #  ie, if this tag exists, it's value will be used as the routing key
  routing_tag = "host"

  # InfluxDB retention policy
  #retention_policy = "default"
  # InfluxDB database
  #database = "telegraf"
  # InfluxDB precision
  #precision = "s"

# Configuration for DataDog API to send metrics to.
[[outputs.datadog]]
  # Datadog API key
  apikey = "my-secret-key" # required.

  # Connection timeout.
  # timeout = "5s"

# Configuration for influxdb server to send metrics to
[[outputs.influxdb]]
  # The full HTTP or UDP endpoint URL for your InfluxDB instance
  # Multiple urls can be specified for InfluxDB cluster support.
  # urls = ["udp://localhost:8089"] # UDP endpoint example
  urls = ["http://localhost:8086"] # required
  # The target database for metrics (telegraf will create it if not exists)
  database = "telegraf" # required
  # Precision of writes, valid values are n, u, ms, s, m, and h
  # note: using second precision greatly helps InfluxDB compression
  precision = "s"

  # Connection timeout (for the connection with InfluxDB), formatted as a string.
  # If not provided, will default to 0 (no timeout)
  # timeout = "5s"
  # username = "telegraf"
  # password = "metricsmetricsmetricsmetrics"
  # Set the user agent for HTTP POSTs (can be useful for log differentiation)
  # user_agent = "telegraf"
  # Set UDP payload size, defaults to InfluxDB UDP Client default (512 bytes)
  # udp_payload = 512

# Configuration for the Kafka server to send metrics to
[[outputs.kafka]]
  # URLs of kafka brokers
  brokers = ["localhost:9092"]
  # Kafka topic for producer messages
  topic = "telegraf"
  # Telegraf tag to use as a routing key
  #  ie, if this tag exists, it's value will be used as the routing key
  routing_tag = "host"

# Configuration for Librato API to send metrics to.
[[outputs.librato]]
  # Librator API Docs
  # http://dev.librato.com/v1/metrics-authentication

  # Librato API user
  api_user = "telegraf@influxdb.com" # required.

  # Librato API token
  api_token = "my-secret-token" # required.

  # Tag Field to populate source attribute (optional)
  # This is typically the _hostname_ from which the metric was obtained.
  source_tag = "hostname"

  # Connection timeout.
  # timeout = "5s"

# Configuration for MQTT server to send metrics to
[[outputs.mqtt]]
  servers = ["localhost:1883"] # required.

  # MQTT outputs send metrics to this topic format
  #    "<topic_prefix>/host/<hostname>/<pluginname>/"
  #   ex: prefix/host/web01.example.com/mem/available
  # topic_prefix = "prefix"

  # username and password to connect MQTT server.
  # username = "telegraf"
  # password = "metricsmetricsmetricsmetrics"

# Send telegraf measurements to NSQD
[[outputs.nsq]]
  # Location of nsqd instance listening on TCP
  server = "localhost:4150"
  # NSQ topic for producer messages
  topic = "telegraf"

# Configuration for OpenTSDB server to send metrics to
[[outputs.opentsdb]]
  # prefix for metrics keys
  prefix = "my.specific.prefix."

  ## Telnet Mode ##
  # DNS name of the OpenTSDB server in telnet mode
  host = "opentsdb.example.com"

  # Port of the OpenTSDB server in telnet mode
  port = 4242

  # Debug true - Prints OpenTSDB communication
  debug = false

# Configuration for the Prometheus client to spawn
[[outputs.prometheus_client]]
  # Address to listen on
  # listen = ":9126"


###############################################################################
#                                  PLUGINS                                    #
###############################################################################

# Read stats from an aerospike server
[aerospike]
  # Aerospike servers to connect to (with port)
  # Default: servers = ["localhost:3000"]
  #
  # This plugin will query all namespaces the aerospike
  # server has configured and get stats for them.
  servers = ["localhost:3000"]

# Read Apache status information (mod_status)
[apache]
  # An array of Apache status URI to gather stats.
  urls = ["http://localhost/server-status?auto"]

# Read metrics of bcache from stats_total and dirty_data
[bcache]
  # Bcache sets path
  # If not specified, then default is:
  # bcachePath = "/sys/fs/bcache"
  #
  # By default, telegraf gather stats for all bcache devices
  # Setting devices will restrict the stats to the specified
  # bcache devices.
  # bcacheDevs = ["bcache0", ...]

# Read metrics about cpu usage
[cpu]
  # Whether to report per-cpu stats or not
  percpu = true
  # Whether to report total system cpu stats or not
  totalcpu = true
  # Comment this line if you want the raw CPU time metrics
  drop = ["cpu_time"]

# Read metrics about disk usage by mount point
[disk]
  # By default, telegraf gather stats for all mountpoints.
  # Setting mountpoints will restrict the stats to the specified mountpoints.
  # Mountpoints=["/"]

# Read metrics from one or many disque servers
[disque]
  # An array of URI to gather stats about. Specify an ip or hostname
  # with optional port and password. ie disque://localhost, disque://10.10.3.33:18832,
  # 10.0.0.1:10000, etc.
  #
  # If no servers are specified, then localhost is used as the host.
  servers = ["localhost"]

# Read metrics about docker containers
[docker]
  # no configuration

# Read stats from one or more Elasticsearch servers or clusters
[elasticsearch]
  # specify a list of one or more Elasticsearch servers
  servers = ["http://localhost:9200"]

  # set local to false when you want to read the indices stats from all nodes
  # within the cluster
  local = true

  # set cluster_health to true when you want to also obtain cluster level stats
  cluster_health = false

# Read flattened metrics from one or more commands that output JSON to stdout
[exec]
  # specify commands via an array of tables
  [[exec.commands]]
  # the command to run
  command = "/usr/bin/mycollector --foo=bar"

  # name of the command (used as a prefix for measurements)
  name = "mycollector"

  # Only run this command if it has been at least this many
  # seconds since it last ran
  interval = 10

# Read metrics of haproxy, via socket or csv stats page
[haproxy]
  # An array of address to gather stats about. Specify an ip on hostname
  # with optional port. ie localhost, 10.10.3.33:1936, etc.
  #
  # If no servers are specified, then default to 127.0.0.1:1936
  servers = ["http://myhaproxy.com:1936", "http://anotherhaproxy.com:1936"]
  # Or you can also use local socket(not work yet)
  # servers = ["socket:/run/haproxy/admin.sock"]

# Read flattened metrics from one or more JSON HTTP endpoints
[httpjson]
  # Specify services via an array of tables
  [[httpjson.services]]

    # a name for the service being polled
    name = "webserver_stats"

    # URL of each server in the service's cluster
    servers = [
      "http://localhost:9999/stats/",
      "http://localhost:9998/stats/",
    ]

    # HTTP method to use (case-sensitive)
    method = "GET"

    # List of tag names to extract from top-level of JSON server response
    # tag_keys = [
    #   "my_tag_1",
    #   "my_tag_2"
    # ]

    # HTTP parameters (all values must be strings)
    [httpjson.services.parameters]
      event_type = "cpu_spike"
      threshold = "0.75"

# Read metrics about disk IO by device
[io]
  # By default, telegraf will gather stats for all devices including
  # disk partitions.
  # Setting devices will restrict the stats to the specified devcies.
  # Devices=["sda","sdb"]
  # Uncomment the following line if you do not need disk serial numbers.
  # SkipSerialNumber = true

# Read JMX metrics through Jolokia
[jolokia]
  # This is the context root used to compose the jolokia url
  context = "/jolokia/read"

  # Tags added to each measurements
  [jolokia.tags]
    group = "as"

  # List of servers exposing jolokia read service
  [[jolokia.servers]]
    name = "stable"
    host = "192.168.103.2"
    port = "8180"

  # List of metrics collected on above servers
  # Each metric consists in a name, a jmx path and either a pass or drop slice attributes
  # This collect all heap memory usage metrics
  [[jolokia.metrics]]
    name = "heap_memory_usage"
    jmx  = "/java.lang:type=Memory/HeapMemoryUsage"


  # This drops the 'committed' value from Eden space measurement
  [[jolokia.metrics]]
    name = "memory_eden"
    jmx  = "/java.lang:type=MemoryPool,name=PS Eden Space/Usage"
    drop = [ "committed" ]


  # This passes only DaemonThreadCount and ThreadCount
  [[jolokia.metrics]]
    name = "heap_threads"
    jmx  = "/java.lang:type=Threading"
    pass = [
      "DaemonThreadCount",
      "ThreadCount"
    ]

# read metrics from a Kafka topic
[kafka]
  # topic to consume
  topic = "topic_with_metrics"

  # the name of the consumer group
  consumerGroupName = "telegraf_metrics_consumers"

  # an array of Zookeeper connection strings
  zookeeperPeers = ["localhost:2181"]

  # Batch size of points sent to InfluxDB
  batchSize = 1000

# Read metrics from a LeoFS Server via SNMP
[leofs]
  # An array of URI to gather stats about LeoFS.
  # Specify an ip or hostname with port. ie 127.0.0.1:4020
  #
  # If no servers are specified, then 127.0.0.1 is used as the host and 4020 as the port.
  servers = ["127.0.0.1:4021"]

# Read metrics from local Lustre service on OST, MDS
[lustre2]
  # An array of /proc globs to search for Lustre stats
  # If not specified, the default will work on Lustre 2.5.x
  #
  # ost_procfiles = ["/proc/fs/lustre/obdfilter/*/stats", "/proc/fs/lustre/osd-ldiskfs/*/stats"]
  # mds_procfiles = ["/proc/fs/lustre/mdt/*/md_stats"]

# Read metrics about memory usage
[mem]
  # no configuration

# Read metrics from one or many memcached servers
[memcached]
  # An array of address to gather stats about. Specify an ip on hostname
  # with optional port. ie localhost, 10.0.0.1:11211, etc.
  #
  # If no servers are specified, then localhost is used as the host.
  servers = ["localhost"]

# Read metrics from one or many MongoDB servers
[mongodb]
  # An array of URI to gather stats about. Specify an ip or hostname
  # with optional port add password. ie mongodb://user:auth_key@10.10.3.30:27017,
  # mongodb://10.10.3.33:18832, 10.0.0.1:10000, etc.
  #
  # If no servers are specified, then 127.0.0.1 is used as the host and 27107 as the port.
  servers = ["127.0.0.1:27017"]

# Read metrics from one or many mysql servers
[mysql]
  # specify servers via a url matching:
  #  [username[:password]@][protocol[(address)]]/[?tls=[true|false|skip-verify]]
  #  see https://github.com/go-sql-driver/mysql#dsn-data-source-name
  #  e.g.
  #    root:passwd@tcp(127.0.0.1:3306)/?tls=false
  #    root@tcp(127.0.0.1:3306)/?tls=false
  #
  # If no servers are specified, then localhost is used as the host.
  servers = ["tcp(127.0.0.1:3306)/"]

# Read metrics about network interface usage
[net]
  # By default, telegraf gathers stats from any up interface (excluding loopback)
  # Setting interfaces will tell it to gather these explicit interfaces,
  # regardless of status.
  #
  # interfaces = ["eth0", ... ]

# Read metrics about TCP status such as established, time wait etc and UDP sockets counts.
[netstat]
  # no configuration

# Read Nginx's basic status information (ngx_http_stub_status_module)
[nginx]
  # An array of Nginx stub_status URI to gather stats.
  urls = ["http://localhost/status"]

# Read metrics of phpfpm, via HTTP status page or socket(pending)
[phpfpm]
  # An array of addresses to gather stats about. Specify an ip or hostname
  # with optional port and path.
  #
  # Plugin can be configured in three modes (both can be used):
  #   - http: the URL must start with http:// or https://, ex:
  #       "http://localhost/status"
  #       "http://192.168.130.1/status?full"
  #   - unixsocket: path to fpm socket, ex:
  #       "/var/run/php5-fpm.sock"
  #       "192.168.10.10:/var/run/php5-fpm-www2.sock"
  #   - fcgi: the URL mush start with fcgi:// or cgi://, and port must present, ex:
  #       "fcgi://10.0.0.12:9000/status"
  #       "cgi://10.0.10.12:9001/status"
  #
  # If no servers are specified, then default to 127.0.0.1/server-status
  urls = ["http://localhost/status"]

# Ping given url(s) and return statistics
[ping]
  # urls to ping
  urls = ["www.google.com"] # required
  # number of pings to send (ping -c <COUNT>)
  count = 1 # required
  # interval, in s, at which to ping. 0 == default (ping -i <PING_INTERVAL>)
  ping_interval = 0.0
  # ping timeout, in s. 0 == no timeout (ping -t <TIMEOUT>)
  timeout = 0.0
  # interface to send ping from (ping -I <INTERFACE>)
  interface = ""

# Read metrics from one or many postgresql servers
[postgresql]
  # specify servers via an array of tables
  [[postgresql.servers]]

  # specify address via a url matching:
  #   postgres://[pqgotest[:password]]@localhost[/dbname]?sslmode=[disable|verify-ca|verify-full]
  # or a simple string:
  #   host=localhost user=pqotest password=... sslmode=... dbname=app_production
  #
  # All connection parameters are optional. By default, the host is localhost
  # and the user is the currently running user. For localhost, we default
  # to sslmode=disable as well.
  #
  # Without the dbname parameter, the driver will default to a database
  # with the same name as the user. This dbname is just for instantiating a
  # connection with the server and doesn't restrict the databases we are trying
  # to grab metrics for.
  #

  address = "sslmode=disable"

  # A list of databases to pull metrics about. If not specified, metrics for all
  # databases are gathered.

  # databases = ["app_production", "blah_testing"]

  # [[postgresql.servers]]
  # address = "influx@remoteserver"

# Monitor process cpu and memory usage
[procstat]
  [[procstat.specifications]]
  prefix = "" # optional string to prefix measurements
  # Use one of pid_file or exe to find process
  pid_file = "/var/run/nginx.pid"
  # executable name (used by pgrep)
  # exe = "nginx"

# Read metrics from one or many prometheus clients
[prometheus]
  # An array of urls to scrape metrics from.
  urls = ["http://localhost:9100/metrics"]

# Reads last_run_summary.yaml file and converts to measurments
[puppetagent]
  # Location of puppet last run summary file
  location = "/var/lib/puppet/state/last_run_summary.yaml"

# Read metrics from one or many RabbitMQ servers via the management API
[rabbitmq]
  # Specify servers via an array of tables
  [[rabbitmq.servers]]
  # name = "rmq-server-1" # optional tag
  # url = "http://localhost:15672"
  # username = "guest"
  # password = "guest"

  # A list of nodes to pull metrics about. If not specified, metrics for
  # all nodes are gathered.
  # nodes = ["rabbit@node1", "rabbit@node2"]

# Read metrics from one or many redis servers
[redis]
  # specify servers via a url matching:
  #  [protocol://][:password]@address[:port]
  #  e.g.
  #    tcp://localhost:6379
  #    tcp://:password@192.168.99.100
  #
  # If no servers are specified, then localhost is used as the host.
  # If no port is specified, 6379 is used
  servers = ["tcp://localhost:6379"]

# Read metrics from one or many RethinkDB servers
[rethinkdb]
  # An array of URI to gather stats about. Specify an ip or hostname
  # with optional port add password. ie rethinkdb://user:auth_key@10.10.3.30:28105,
  # rethinkdb://10.10.3.33:18832, 10.0.0.1:10000, etc.
  #
  # If no servers are specified, then 127.0.0.1 is used as the host and 28015 as the port.
  servers = ["127.0.0.1:28015"]

# Read metrics about swap memory usage
[swap]
  # no configuration

# Read metrics about system load & uptime
[system]
  # no configuration

# Read Twemproxy stats data
[twemproxy]
  [[twemproxy.instances]]
    # Twemproxy stats address and port (no scheme)
    addr = "localhost:22222"
    # Monitor pool name
    pools = ["redis_pool", "mc_pool"]

# Read metrics of ZFS from arcstats, zfetchstats and vdev_cache_stats
[zfs]
  # ZFS kstat path
  # If not specified, then default is:
  # kstatPath = "/proc/spl/kstat/zfs"
  #
  # By default, telegraf gather all zfs stats
  # If not specified, then default is:
  # kstatMetrics = ["arcstats", "zfetchstats", "vdev_cache_stats"]

# Reads 'mntr' stats from one or many zookeeper servers
[zookeeper]
  # An array of address to gather stats about. Specify an ip or hostname
  # with port. ie localhost:2181, 10.0.0.1:2181, etc.

  # If no servers are specified, then localhost is used as the host.
  # If no port is specified, 2181 is used
  servers = [":2181"]


###############################################################################
#                              SERVICE PLUGINS                                #
###############################################################################

# Statsd Server
[statsd]
  # Address and port to host UDP listener on
  service_address = ":8125"
  # Delete gauges every interval (default=false)
  delete_gauges = false
  # Delete counters every interval (default=false)
  delete_counters = false
  # Delete sets every interval (default=false)
  delete_sets = false
  # Delete timings & histograms every interval (default=true)
  delete_timings = true
  # Percentiles to calculate for timing & histogram stats
  percentiles = [90]

  # templates = [
  #     "cpu.* measurement*"
  # ]

  # Number of UDP messages allowed to queue up, once filled,
  # the statsd server will start dropping packets
  allowed_pending_messages = 10000

  # Number of timing/histogram values to track per-measurement in the
  # calculation of percentiles. Raising this limit increases the accuracy
  # of percentiles but also increases the memory usage and cpu time.
  percentile_limit = 1000
